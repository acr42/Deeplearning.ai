{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function computes error for a single example and cost function is averafe cist if the functions across all examples\n",
    "\n",
    "$loss\\ function: \\ \\mathcal{L}(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2 $\n",
    "\n",
    "$log \\ loss\\ function: \\ \\mathcal{L}(\\hat{y},y) = -ylog(\\hat{y}) + (1-y)log(1-\\hat{y})$\n",
    "\n",
    "$cost\\ function: \\ \\mathcal{J}(w,b) = \\frac{1}{m} \\sum^m_{i=1}L(\\hat{y}^(i),y^{(i)})$\n",
    "\n",
    "If convex cost function (i.e bowl), use gradient descent with ease \n",
    "\n",
    "$\\mathcal{w}:=\\mathcal{w}-\\alpha \\frac{\\mathcal{dJ(w)}}{dw}=w-\\alpha dw$ where $\\alpha \\ and\\ \\mathcal{dw}=\\frac{\\mathcal{dJ(w)}}{\\mathcal{dw}}$ is the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised version takes 71.70391082763672 ms\n",
      "Nonvectorised version takes 603.6019325256348 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "a=np.random.rand(1000000)\n",
    "b=np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c=np.dot(a,b)\n",
    "toc=time.time()\n",
    "\n",
    "print(\"Vectorised version takes \"+str(1000*(toc-tic))+\" ms\")\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c+=a[i]*b[i]\n",
    "toc=time.time()\n",
    "\n",
    "print(\"Nonvectorised version takes \"+str(1000*(toc-tic))+\" ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is [[101 102 103]\n",
      " [103 104 105]] \n",
      "   \n",
      "aggregated  is [204 206 208]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[1,2,3],[3,4,5]])\n",
    "A=A+100 #broadcasting\n",
    "print(\"A is {} \".format(A)) # addingg in values to string\n",
    "cal=A.sum(axis=0)\n",
    "print(\"   \")\n",
    "\n",
    "print(\"aggregated  is\",cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 49.50980392  49.51456311  49.51923077]\n",
      " [ 50.49019608  50.48543689  50.48076923]]\n"
     ]
    }
   ],
   "source": [
    "percentage=A*100/cal.reshape(1,3)\n",
    "print(percentage)\n",
    "#error is max likelyhood estimation of -L(y1,y2), same as minimising L(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a = np.ones((4,3)) # a.shape = (4, 3)\n",
    "# b = np.random.randn(4, 1) # b.shape = (3, 2)\n",
    "b = np.asarray([[1],[2],[3],[4]])\n",
    "c = a*b #* and + are always element wise, use np.dot for dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "\n",
    "Components of a neural net\n",
    " \n",
    "- Input \n",
    "- Hidden layer \n",
    "- Output Layer \n",
    "\n",
    "Equations Within Net\n",
    "\n",
    "\n",
    "$\\underline{a}^{[0]}=\\underline{X}$ - This is the first layer of activation function - x gets passed to hiddden layer\n",
    "\n",
    "$\\underline{a}^{[1]}=\\underline{X}$ - This is the first hidden layer units after one application of activation function\n",
    "\n",
    "$z=W^{T}x+b$\n",
    "\n",
    "$a=\\sigma (z)$\n",
    "\n",
    "never use sigmoid activation function, use tanh apart from last layer where we may need num between 0 and 1 then use sig\n",
    "\n",
    "look into leaky relu activation function\n",
    "\n",
    "derivative of tanhx = 1-tanhx^2\n",
    "\n",
    "derivative (ReLU) of max(0,z)=0 if z < 0 and 1 if z > 0\n",
    "\n",
    "derivative (Leaky ReLU) of max(0.01z,z)=0.01 if z < 0 and 1 if z > 0\n",
    "\n",
    "\n",
    "#look into doing a GAN project with Sebastien \n",
    "\n",
    "when initialise weights use np.rand((2,2))*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([[1,2,3],[1,2,3]]),axis=1,keepdims=True).shape #keepdims=true keeps end as (2,1) and not (2,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "ReLU - much quikcer but less powerfull than tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
