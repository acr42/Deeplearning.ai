{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function computes error for a single example and cost function is averafe cist if the functions across all examples\n",
    "\n",
    "$loss\\ function: \\ \\mathcal{L}(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2 $\n",
    "\n",
    "$log \\ loss\\ function: \\ \\mathcal{L}(\\hat{y},y) = -ylog(\\hat{y}) + (1-y)log(1-\\hat{y})$\n",
    "\n",
    "$cost\\ function: \\ \\mathcal{J}(w,b) = \\frac{1}{m} \\sum^m_{i=1}L(\\hat{y}^(i),y^{(i)})$\n",
    "\n",
    "If convex cost function (i.e bowl), use gradient descent with ease \n",
    "\n",
    "$\\mathcal{w}:=\\mathcal{w}-\\alpha \\frac{\\mathcal{dJ(w)}}{dw}=w-\\alpha dw$ where $\\alpha \\ and\\ \\mathcal{dw}=\\frac{\\mathcal{dJ(w)}}{\\mathcal{dw}}$ is the learning rate\n",
    "\n",
    "$\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised version takes 1.9838809967041016 ms\n",
      "Nonvectorised version takes 488.89660835266113 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "a=np.random.rand(1000000)\n",
    "b=np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c=np.dot(a,b)\n",
    "toc=time.time()\n",
    "\n",
    "print(\"Vectorised version takes \"+str(1000*(toc-tic))+\" ms\")\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c+=a[i]*b[i]\n",
    "toc=time.time()\n",
    "\n",
    "print(\"Nonvectorised version takes \"+str(1000*(toc-tic))+\" ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "\n",
    "Components of a neural net\n",
    " \n",
    "- Input \n",
    "- Hidden layer \n",
    "- Output Layer \n",
    "\n",
    "Equations Within Net\n",
    "\n",
    "\n",
    "$\\underline{a}^{[0]}=\\underline{X}$ - This is the first layer of activation function - x gets passed to hiddden layer\n",
    "\n",
    "$\\underline{a}^{[1]}=\\underline{X}$ - This is the first hidden layer units after one application of activation function\n",
    "\n",
    "$z=W^{T}x+b$\n",
    "\n",
    "$a=\\sigma (z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
